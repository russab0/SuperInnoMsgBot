# -*- coding: utf-8 -*-
"""Language model template.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R5ad-9hLa4VnyJdeGLTwkP7olJ95jSKP

# Language model

Language model is a probability distribution over sequences of word.

In this lab we will apply laguage model for a classification problem. The task is to implement a filter for spam documents.

Read this article
https://towardsdatascience.com/learning-nlp-language-models-with-real-data-cdff04c51c25

### Dataset
Download this https://www.kaggle.com/uciml/sms-spam-collection-dataset dataset.
Normalize the text and split by sentences using nltk library. Split sentences to the terms. We don't need to do lemmatize words and remove stop words. For simplicity we will lose the punctuation and characters register.
Make a lists of sentences for spam and ham messages.
"""

import pandas as pd
import nltk

from itertools import chain
import re

df = pd.read_csv('spam.csv', encoding = 'ISO-8859-1')[['v1', 'v2']]
df.rename(columns={'v1': 'type', 'v2': 'content'}, inplace=True)
df.head()

spam_messages = df[df['type'] == 'spam']['content'].tolist() #list of sentences, each sentence represented as a list of terms
ham_messages = df[df['type'] == 'ham']['content'].tolist()

len(spam_messages), len(ham_messages)

spam_messages[0:5]

"""Print the average length and average number of sentences in spam message."""

from nltk.tokenize import sent_tokenize, word_tokenize
_ = nltk.download('punkt')

sent_sizes = [len(word_tokenize(sent)) for msg in spam_messages for sent in sent_tokenize(msg)]
avg_sent_len = sum(sent_sizes) / len(sent_sizes)

avg_sent_num = sum([len(sent_tokenize(msg)) for msg in spam_messages]) / len(spam_messages)

print(f'Average sentence length: {avg_sent_len:.2f}, average num of sent: {avg_sent_num:.2f}')

import string

PUNCT = string.punctuation + '“”'

def normalize(text, allow_asterix=False):
    text = text.lower()
    text = re.sub('\'', '', text)                          # remove apostrophes
    #text = re.sub('[!\?@#$.\-+=><—,\(\)&:“”]', ' ', text)        # replace all punctuation signs with spaces
    text = re.sub(f'[{PUNCT}]', ' ', text)        # replace all punctuation signs with spaces
    
    if not allow_asterix:
        text = re.sub('\*', ' ', text)                      # replace all astrixes (*) with spaces
    text = re.sub('[0-9]', ' ', text)                      # replace all digits with spaces
    result = " ".join([x.lower() for x in text.split()])   # lower all letters and delete all doubled spaces
    return result

def messages_to_sentences(messages):
    return list(chain(
        *[[word_tokenize(normalize(sent)) for sent in sent_tokenize(msg)] for msg in messages]))

spam_sentences = messages_to_sentences(spam_messages)
ham_sentences = messages_to_sentences(ham_messages)

for i in range(10):
    print(spam_sentences[i])

"""### Unigram model

Calculate the number of occurancies of each term separately for spam and ham messages. 

Calculate the total number of terms.
"""

class CountDict(dict):
    """
    Class that is used as counter, inherited from dict. 
    """
    def __getitem__(self, item):
        """
        Gets item without exceptions
        :param item:  key you want to get
        :return: value, associated with `item` (if item in the `keys`), 0 otherwise
        """
        if item not in self:
            return 0
        return super().__getitem__(item)

from collections import Counter

START = 'START'
END = 'END'

spam_term_c = CountDict(Counter(list(chain(*spam_sentences))))  # dict()
spam_term_c[START] = spam_term_c[END] = len(spam_sentences)
spam_N = sum(spam_term_c.values())

ham_term_c = CountDict(Counter(list(chain(*ham_sentences))))  # dict()
ham_term_c[START] = ham_term_c[END] = len(ham_sentences)
ham_N = sum(ham_term_c.values())

spam_N, ham_N

"""Print 10 most popular words in spam messages."""

'Spam most popular words', spam_term_c.most_common(10)

'Ham most popular words', ham_term_c.most_common(10)

"""### Bigram model

We will use sentence begining and sentence ending as a special terms. Calculate the number of occuracnies for bigrams. As a key in dictionary you might use words, separated by the space symbol.

Also, for a genetative model, epxlained later, for each term we will need a list of next term, found in the dataset.
"""

def bigram_nextword(sentences):
    bigrams = CountDict()
    next_words = dict()
    for i in range(len(sentences)):
        sent = [START] + sentences[i] + [END]
        for cur_token_pos in range(len(sent) - 1):
            cur_token, next_token = sent[cur_token_pos:cur_token_pos + 2]
            bigram = cur_token + ' ' + next_token
            if not bigram in bigrams:
                bigrams[bigram] = 0
            bigrams[bigram] += 1
            
            if not cur_token in next_words:
                next_words[cur_token] = set()
            next_words[cur_token].add(next_token)
    for k in next_words:
        next_words[k] = list(next_words[k]) 
    return bigrams, next_words

spam_bigram_c, spam_next_words = bigram_nextword(spam_sentences)
ham_bigram_c, _ = bigram_nextword(spam_sentences)

spam_bi_N = sum(spam_bigram_c.values())
ham_bi_N = sum(ham_bigram_c.values())

"""Which bigrams are the most popular in spam messages?

From which words spam sentence usually begins?
"""

spam_bigram_pop = sorted(list(spam_bigram_c.items()), 
                         key=lambda x: x[1],
                         reverse=True
                         )[:10]

spam_bigram_start_pop = sorted(
    [x for x in spam_bigram_c.items() if x[0].startswith(START)],
    key=lambda x: x[1],
    reverse=True
    )[:10]

spam_bigram_pop, spam_bigram_start_pop

"""Implement a function, which return the conditional probability $P(t_2 | t_1) = \frac{count(t_1 t_2)}{count(t_1)}$"""

def conditional_prob(t1, t2, spam=True):
    bigram = t1 + ' ' + t2
    if spam:
        if spam_bigram_c[bigram] * spam_term_c[t1] == 0:
            return 0
        return spam_bigram_c[bigram] / spam_term_c[t1]
    else:
        #print(t1, t2, ham_bigram_c[bigram], ham_term_c[t1])
        if ham_bigram_c[bigram] * ham_term_c[t1] == 0:
            return 0
        return ham_bigram_c[bigram] / ham_term_c[t1]

"""### Genetative model

Now is the funny task. Using your language model generate a spam message. Remember you calculated the average number of sentences, average sentence size for spam messages.

Print few generated ouptuts.

#### Interesting outputs

- `Urgent. Reply or å£ cash every week. You are trying to no prepayment`
- `Win a guaranteed. Txt chat on orange line rental camcorder hit. Stop to contact u been renewed and downloads`
- `Free top quality ringtone club credits pls reply. Just txt great graphics from landline box croydon. Urgent`
"""

from random import random, choice, seed
import numpy as np

seed(1)
np.random.seed(1)

def generate_sentence(bigrams, next_words, sent_size):
    sent = []
    prev_word = START
    for j in range(round(sent_size)):
        if prev_word == END:
            break
        possible = next_words[prev_word]
        pdf = np.array([conditional_prob(prev_word, word) for word in possible])
        cdf = np.cumsum(pdf)

        cur_word = possible[0]
        r = random()
        for i in range(len(cdf)):
            if cdf[i] > r:
                cur_word = possible[i]
                break
        sent.append(cur_word)
        prev_word = cur_word
        
    sent[0] = sent[0].capitalize()
    return ' '.join(sent[:-1])

def generate(bigrams, next_words, sent_size, sent_num):
    text = [
            generate_sentence(bigrams, next_words, sent_size)
            for _ in range(round(sent_num))
    ]
    return '. '.join(text)

for i in range(5):
    print(generate(spam_bigram_c, spam_next_words, avg_sent_len, avg_sent_num))

"""### Smoothing

The problem is that if the bigram $t_1 t_2$ occuted $0$ times in the corpus, the conditional probability $P(t_2|t_1) = 0$

The solution is smoothing. Read this document https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf

Your task is to implement one of the advanced (from the document, except additive smoothing) smoothing techniques from it. Be ready to explain it defending the lab.

Implement a function, which return the conditional probability $P(t_2 | t_1)$ with a smoothing.

### Jelinek-Mercer smoothing (interpolation)

$P(t_2 | t_1) = \lambda \cdot \frac{count(t_1 t_2)}{count(t_1)} + (1 - \lambda) \cdot \frac{count(t_1)}{\sum_{i=0}^{M} count(t_i)} $
"""

def bigram_prob(t1, t2, spam=True):
    return conditional_prob(t1, t2, spam)

def unigram_prob(t, spam=True):
    if spam:
        return spam_term_c[t] / spam_N
    else:
        return ham_term_c[t] / ham_N

def smoothing_conditional_prob(t1, t2, spam=True, lam=0.5):
    # Jelinek-Mercer smoothing (interpolation)
    return lamb * bigram_prob(t1, t2, spam) + (1 - lamb) * unigram_prob(t2, spam)


t1, t2 = choice(list(spam_bigram_c.keys())).split()
t1, t2,
for lamb in np.linspace(0, 1, num=11):
    print(f'lambda= {lamb:.1f}, smoothing_cond_prob= {smoothing_conditional_prob(t1, t2, True, lamb):.2f}')

"""### Classification

Now, implement a bayessian classifier for the sentence. Test one of your generated sentences on it.

It should return, which probability is higher

$$P(spam|t_1, \dots , t_k) = \frac{P(t_1, \dots , t_k|spam)P(spam)}{P(t_1, \dots , t_k)} \sim P(t_1, \dots , t_k|spam)P(spam)$$ 
$$\sim P(t_1 | BEGIN, spam) \cdot \sim P(t_2 | t_1, spam) \cdot \dots \cdot \sim P(END | t_k, spam)$$

or the same for ham sentence.
"""

def classification_prob(sentence, spam):
    if isinstance(sentence, str):
        sentence = word_tokenize(sentence)
    sentence_bigrams = [(sentence[i], sentence[i + 1])
                        for i in range(len(sentence) - 1)]
    return np.prod([
                    smoothing_conditional_prob(bi[0], bi[1], spam)
                    for bi in sentence_bigrams
    ])


def classify(sentence):
    print('spam', classification_prob(sentence, spam=True))
    print('ham', classification_prob(sentence, spam=False))
    return None

classify(spam_sentences[1])
classify(spam_sentences[100])

ham_sentences[1], classify(ham_sentences[1])
ham_sentences[100], classify(ham_sentences[100])